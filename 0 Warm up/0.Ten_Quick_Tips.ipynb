{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ten Quick Tips for Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from: \n",
    "**Ten quick tips for Machine Learning in Computational Biology** [paper](https://biodatamining.biomedcentral.com/articles/10.1186/s13040-017-0155-3)\n",
    "\n",
    "<span class=\"fn\"><i>[REF]</i>: Chicco, D. Ten quick tips for machine learning in computational biology. BioData Mining 10, 35 (2017). https://doi.org/10.1186/s13040-017-0155-3</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "*(from the paper)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Recent advances in high-throughput sequencing technologies have made large biological datasets available to the scientific community. \n",
    ">\n",
    "> Together with the growth of these datasets, internet web services expanded, and enabled biologists to put large data online for scientific audiences.\n",
    "> \n",
    "> As a result, scientists have begun to search for novel ways to interrogate, analyze, and process data [$\\ldots$]\n",
    "\n",
    "> A machine learning algorithm is a computational method based upon statistics, implemented in software, able to discover hidden non-obvious patterns in a dataset, and moreover to make reliable statistical predictions about similar new data. \n",
    "> \n",
    "\n",
    "As explained by _Kevin Yip and colleagues_ : \n",
    "\n",
    "> “The ability [of machine learning] to automatically identify patterns in data [$\\ldots$] is particularly important when the expert knowledge is incomplete or inaccurate, when the amount of available data is too large to be handled manually, or when there are exceptions to the general cases”\n",
    "[\\[$1$\\]](#fn1). \n",
    "\n",
    "**This is clearly the case for computational biology and bioinformatics.**\n",
    "\n",
    "> Machine learning (*as well as Deep Learning*, e.d.) has thus been applied to multiple computational biology problems [$\\ldots$]. Despite its importance, often researchers with biology or healthcare backgrounds do not have the specific skills to run a data mining project. \n",
    "> This lack of skills (could) result in incorrect practices, which lead to error-prone analyses, or give them the illusion of success.\n",
    "\n",
    "<span id=\"fn1\"><i>[1]</i>: Yip KY, Cheng C, Gerstein M. Machine learning and genome annotation: a match meant to be? Genome Biol. 2013; 14(5):205.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decalogue \n",
    "\n",
    "(*in other words:* Ten simple rules to keep in mind, whenever possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"top\"></a>\n",
    "1.  [T1](#t1)  Check and arrange your input dataset properly \n",
    "2.  [T2](#t2)  Split your input dataset into three independent subsets \n",
    "3.  [T3](#t3)  Frame your biological problem into the right algorithm category \n",
    "4.  [T4](#t4)  Which algorithm should you choose to start?\n",
    "5.  [T5](#t5)  Take care of the imbalanced data problem \n",
    "6.  [T6](#t6)  Optimize each hyper-parameter \n",
    "7.  [T7](#t7)  Minimize overfitting \n",
    "8.  [T8](#t8)  Evaluate your algorithm performance using the right metric \n",
    "9.  [T9](#t9)  Program your software with open source code and platforms \n",
    "10. [T10](#t10) Ask for feedback and engage with the community "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"t1\"></a>\n",
    "### T1. Check and arrange your input dataset properly\n",
    "\n",
    ">Even though it might seem surprising, the most important key point of a machine learning project does not regard machine learning: it regards your dataset properties and arrangement.\n",
    "\n",
    "This is indeed the pillar of **Data Science**\n",
    "\n",
    "After addressing the issue of the dataset size, the most important priority of your project is the dataset arrangement.\n",
    "\n",
    "<img src=\"toy_pipeline.png\" class=\"maxw80\" alt=\"Toy ML Pipeline\" />\n",
    "\n",
    "\n",
    "###### MLOps (Machine Learning in Production)\n",
    "\n",
    "<img src=\"complex_pipeline.png\" class=\"maxw80\" alt=\"Full-fledged ML Pipeline\" />\n",
    "\n",
    "##### So.. how to arrange data properly?\n",
    "\n",
    "We will see that indeed the relationships that exists between **Machine Learning** and **Data** \n",
    "is quite the point! \n",
    "\n",
    "Therefore, we need to really understand what does it mean to _learn from data_. \n",
    "\n",
    "> This is true in general, and particularly true for the _Bio/Med_ context.\n",
    "\n",
    "Therefore, it will be important to understant:\n",
    "\n",
    "- _how to **represent** data_ for Machine learning;\n",
    "- _how to **use** data_ in (different) Machine learning settings.\n",
    "\n",
    "This topic will be analysed from two angles, shortly summarised in the following two \n",
    "**Research Questions** (RQ):\n",
    "\n",
    "* **Data for Machine Learning**\n",
    "    - (RQ1): How to represent data for Machine/Deep learning models?\n",
    "    \n",
    "* __Machine Learning for Data__\n",
    "    - (RQ2): Which model should I use for different (types of) data?\n",
    "    (See [T4](#t4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The Idea\n",
    "\n",
    "*They say* :\n",
    "\n",
    "> A picture worths a thousand words\n",
    "\n",
    "(hope this applies to drawings and sketches too)\n",
    "\n",
    "I am going to guide our discussion via drawing sketches in order to finally derive a *mind-map-like* schema that will help us in taking decisions about data and (deep learning) models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data for Machine Learning\n",
    "\n",
    "In this section we will try to find an answer to the following questions:\n",
    "\n",
    "1. How to represent data for machine learning?\n",
    "\n",
    "Before going into the details of this, let's have a brief look at what is the **general framework** in which we are operating.\n",
    "\n",
    "> \"Machine learning is about **mapping data to predictions**\"\n",
    "\n",
    "![from data to predictions](ml_model.png)\n",
    "\n",
    "The ultimate goal of a machine learning model is to discover **general**[$^1$](#fn1) patterns.\n",
    "So, ideally, a `Model` would take in input some `Data`, and will generate a `prediction`.\n",
    "The nature of this prediction depends on the specific learning problem at hand: it can be either the `class` data belongs to, or its corresponding `cluster`. \n",
    "\n",
    "\n",
    "<span id=\"fn1\"><i>[1]: </i>We will better understand the meaning of _general_ pattern, and _generalisation_ of a Machine learning model, working on a concrete example.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A slightly more complete picture\n",
    "\n",
    "However the real _pipeline_ is **never** that simple. \n",
    "\n",
    "It involves a lot of **data science** to _preprocess_ `raw` data in order to **generate** a representation which will be suitable for the `Model`. \n",
    "\n",
    "Moreover, the Model itself will encompass a series of further steps and operations before it will be \n",
    "really exercised to generate predictions. \n",
    "In fact, the Model needs to be deployed on a real system to actually go in production. \n",
    "\n",
    "This usually requires some monitor, logging, and the model to be validated to enable _serving_.\n",
    "\n",
    "<img src=\"ml_full_picture.png\" class=\"maxw100\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how we should **transform** data to make it ready to be processed by a Machine learning `Model`? \n",
    "\n",
    "#### Data Representation for Machine learning\n",
    "\n",
    "Machine learning is about creating models from data: for that reason, we'll start by\n",
    "discussing how data can (or should) be represented in order to be understood by **models**.\n",
    "\n",
    "(With very few exceptions) In Machine learning data is assumed to be stored as a\n",
    "**two-dimensional array**, of size `[n_samples, n_features]`. \n",
    "\n",
    "This array is usually referrred as the **feature matrix**.\n",
    "\n",
    "In case of *Supervised learning settings*, there is also the **label vector**, of size `n_samples`, containing the list of labels\n",
    "for each samples.\n",
    "\n",
    "$$\n",
    "{\\rm feature~matrix:~~~} {\\bf X}~=~\\left[\n",
    "\\begin{matrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1D}\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2D}\\\\\n",
    "x_{31} & x_{32} & \\cdots & x_{3D}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{N1} & x_{N2} & \\cdots & x_{ND}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\rm label~vector:~~~} {\\bf y}~=~ [y_1, y_2, y_3, \\cdots y_N]\n",
    "$$\n",
    "\n",
    "Here there are $N$ samples and $D$ features.\n",
    "\n",
    "- $N$ (`n_samples`):   The number of samples: each sample is an item to process (e.g. classify).\n",
    "  A sample can be a document, a picture, a sound, a video, an astronomical object,\n",
    "  a row in database or CSV file,\n",
    "  or whatever you can describe with a fixed set of quantitative traits.\n",
    "- $D$ (`n_features`):  The number of features or distinct traits that can be used to describe each\n",
    "  item in a quantitative manner.  Features are generally real-valued, but may be boolean or\n",
    "  discrete-valued in some cases.\n",
    "\n",
    "The number of **features** _must_ be fixed in advance.\n",
    "Each sample (data point) is a row in the feature data array, and each feature **may be** a column \n",
    "(if features can be expressed by `1D vector`). \n",
    "\n",
    "Features can be also be very high dimensional (e.g. millions of features), but sometimes also very sparse. This is a case where `scipy.sparse` matrices (and `torch.sparse`) [tensors](https://pytorch.org/docs/stable/sparse.html?highlight=torch%20sparse) can be very useful. \n",
    "These structures are much more memory-efficient than **dense** `numpy` arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Here, the **Supervised Machine Learning** settings is considered, without loss of generality)\n",
    "\n",
    "<img src=\"ml_full_supervised.png\" class=\"maxw100\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"t2\"></a>\n",
    "\n",
    "### T2. Split your input dataset into three independent subsets (training set, validation set, test set), and use the test set only once you complete training and optimization phases\n",
    "\n",
    "\n",
    "#### Training and Test Data\n",
    "\n",
    "\n",
    "To evaluate how well our supervised models generalize, we can split our data into a training and a test set:\n",
    "\n",
    "<img src=\"train_test_split.svg\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Thinking about how machine learning is normally performed, the idea of a train/test split makes sense. \n",
    "\n",
    "* Real world systems train on the data they have, and as other data comes in (from customers, sensors, or other sources) the classifier that was trained must predict on fundamentally *new* data. \n",
    "\n",
    "* We can simulate this during training using a train/test split - the test data is a simulation of \"future data\" which will come into the system during production. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=1999)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By evaluating our classifier performance on data that has been seen during training, we \n",
    "**do get** false confidence in the power of our system. \n",
    "\n",
    "This might lead to putting a system into production which *fails* at predicting new data! \n",
    "\n",
    "It is much better to use a train/test split in order to properly see how your trained model is doing on new data. (**Ultimately, this is the goal of ML**)\n",
    "\n",
    "```python\n",
    "# using sklearn API\n",
    "model.fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "# Accuracy\n",
    "print(np.sum(y_hat == y_test) / len(y_test))\n",
    "```\n",
    "\n",
    "If $\\hat{y}_i$ corresponds to the prediction generated by the model for $i-th$ sample, and $y_i$ the corresponding **expected** target in the ground-truth, the **Accuracy** is defined as:\n",
    "\n",
    "$$\n",
    "\\mathtt{accuracy}(y, \\hat{y}) = \\frac{1}{n_{samples}} \\sum_{i=0}^{n_{samples}} 1(\\hat{y_i} = y_i)\n",
    "$$\n",
    "\n",
    "where $1(x)$ is the **indicator function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation and Scoring Methods\n",
    "\n",
    "A further specialisation of the previous exprimental schema consider a **three-way** split (also suggested as the reference strategy by the TIP)\n",
    "\n",
    "![Train Validation Test](train_validation_test2.svg)\n",
    "\n",
    "However, often (labeled) data is precious, and this approach lets us only use ~3/4 of our data for training. \n",
    "\n",
    "On the other hand, we will only ever try to apply our model 1/4 of our data for testing.\n",
    "\n",
    "A common way to use more of the data to built a model, but also get a more robust estimate of the generalization performance is cross-validation.\n",
    "\n",
    "In cross-validation, the data is split repeatedly into a **training** and **test-set**, with a separate model built for every pair. \n",
    "\n",
    "The test-set scores are then aggregated for a more robust estimate.\n",
    "\n",
    "\n",
    "The most common way to do cross-validation is **k-fold cross-validation**, in which the data is first split into k (often 5 or 10) equal-sized folds, and then for each iteration, one of the k folds is used as test data, and the rest as training data:\n",
    "\n",
    "![Cross Validation Schema](cross_validation.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, each data point will be in the **validation** set exactly once, and we can use all but a `k-th` of the data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"t3\"></a>\n",
    "\n",
    "### T3. Frame your biological problem into the right algorithm category\n",
    "\n",
    "(This is an simple and intuitive (but **yet important**) advice:\n",
    "\n",
    "> You have your biological dataset, your scientific question, and a scientific goal for your project. You have arranged and engineered your dataset, as explained in Tip 1. You decide you want to solve your scientific project with machine learning, but you are undecided about what algorithm to start with.\n",
    "\n",
    "> Before choosing the data mining method, you have to frame your biological problem into the right algorithm category, which will then help you find the right tool to answer your scientific question.\n",
    "\n",
    "**In other words**: It is important to understand how to turn the **biological** problem into a **learning** problem.\n",
    "\n",
    "Once the learning objective is clear, then the (learning) problem has to be properly declined in the \n",
    "most appropriate (*learning*) framework:\n",
    "\n",
    "* Supervised Learning\n",
    "    * Classification\n",
    "    * Regression\n",
    "* Unsupervised Learning\n",
    "    * Clustering\n",
    "    * Dimensionality Reduction\n",
    "\n",
    "[top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"t4\"></a>\n",
    "\n",
    "### T4. Which algorithm should you choose to start?\n",
    "\n",
    "The **Answer** to this question is the most obvious: **The simplest one!**\n",
    "\n",
    ">The **No Free Lunch** theorem states that there is no one model that works best for every problem.  \n",
    ">\n",
    "> The assumptions of a great model for one problem may not hold for another problem, so it is common in machine learning to try multiple models and find one that works best for a particular problem.  \n",
    ">\n",
    "> This is especially true in supervised learning, and cross-validation is commonly used to assess the predictive accuracies of multiple models of varying complexity to find the best model.  A model that works well could also be trained by multiple algorithms – for example, linear regression could be trained by the normal equations or by gradient descent.\n",
    "\n",
    "<span class=\"fn\"><i>[Source]</i>: https://chemicalstatistician.wordpress.com/2014/01/24/machine-learning-lesson-of-the-day-the-no-free-lunch-theorem/ </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is **mostly** to emphasise that **even in the DL** era, classical **ML** methods still have their say, and it is always important to include them in our experimental pipelines (**also because** they're generally super fast to try, if compared to Deep network training).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(from `arXiv` - **11 Jun 2020** )\n",
    "\n",
    "##### Is Deep learning necessary for simple Classification Tasks? \n",
    "\n",
    "(paper [link](https://arxiv.org/abs/2006.06730))\n",
    "\n",
    "> Automated machine learning (`AutoML`) and deep learning (`DL`) are two cutting-edge paradigms \n",
    "> used to solve a myriad of inductive learning tasks. $\\ldots$ \n",
    "> Our observations suggest that `AutoML` outperforms simple `DL` classifiers when trained on similar datasets for binary classification but integrating DL into `AutoML` improves classification performance even further.\n",
    ">\n",
    "> $\\rightarrow$ However, the substantial time needed to train `AutoML+DL` pipelines will likely outweigh performance advantages in many applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"t5\"></a>\n",
    "\n",
    "### T5. Take care of the imbalanced data problem\n",
    "\n",
    "> In computational biology and in biomedicine, it is often common to have **imbalanced** datasets. \n",
    ">\n",
    "> An imbalanced (or unbalanced) dataset is a dataset in which one class is over-represented respect to the other(s)\n",
    "\n",
    "\n",
    "**Some workarounds**:\n",
    "\n",
    "- _class weighting_\n",
    "- Imputation and/or Synth. Data\n",
    "- (at the very least) under-sampling (hardly the case, e.d.)\n",
    "\n",
    "**In addition**:\n",
    "\n",
    "- Use appropriate metrics (see [T8](#t8))\n",
    "- Increase the statistics with Cross-Validation (see [T7](#t7))\n",
    "\n",
    "[top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"t6\"></a>\n",
    "\n",
    "### T.6 Optimise each Hyper-parameter\n",
    "\n",
    "This is the only rule I should argue with.\n",
    "\n",
    "Optimising _each_ hyperparam is **hardly** the case, especially with Deep Learning in which you would try to optimise million of parameters, and plenty of *hyper-parameters*.\n",
    "\n",
    "So, **NO**! Alternatively, though: some **general rule of thumbs** might apply:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [T1](#t1), we explored how Machine learning models expect the data to be represented \n",
    "(i.e. `Data` $\\mapsto$ `Machine Learning`). Here the focus is on the reverse relationship:\n",
    "`Machine Learning` $\\mapsto$ `Data`.\n",
    "\n",
    "In particular, we will try to come up with some practical _rule of thumbs_ that can guide us in the choice of the different _family_ of **Deep Learning** models, given the **nature** and the **type** of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data.png\" class=\"maxw50\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data Shape\n",
    "\n",
    "The first characteristic of the data that we will analyse refers to the **shape** of data, also related to **representation**.\n",
    "\n",
    "Data can be **structured** or **un**structured, and its representation can be **dense** or **sparse** (either is the shape).\n",
    "\n",
    "<img src=\"data_shape.png\" class=\"maxw85\" />\n",
    "\n",
    "###### Data Type\n",
    "\n",
    "<img src=\"data_type.png\" class=\"maxw85\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ML for Data: Choosing the right estimator\n",
    "\n",
    "A very popular page in the **scikit-learn** documentation reports a [guide map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)[$^1$](#dt) containing practical rules and conditions that could be used to choose the right estimator to use.\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_static/ml_map.png\" alt=\"scikit-learn map\" class=\"maxw85\" />\n",
    "\n",
    "\n",
    "<span class=\"fn\" id=\"dt\"><i>[1] :</i> In the form of a Decision Tree</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The (almost) complete picture\n",
    "\n",
    "<img src=\"the_full_map.png\" class=\"maxw100\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as for **Unsupervised learning**, we have options too!\n",
    "\n",
    "* AutoEncoders (`AE`)\n",
    "    - Convolutional `AE`\n",
    "\n",
    "* **Variational** AutoEncoders (`VAE`)\n",
    "\n",
    "* Generative Adversarial Networks (`GAN`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"t7\"></a>\n",
    "\n",
    "### T7. Minimise Overfitting\n",
    "\n",
    "Ultimately, the goal of machine learning algorithms is to be able to **discover patterns** in data. \n",
    "\n",
    "The question is *how to be sure that we have truly learnt a **general** pattern, and not simply memorised our (training) data?*\n",
    "\n",
    ">In order to discuss this phenomenon more formally, \n",
    "we need to differentiate between **training error** and **generalization error**.\n",
    "\n",
    "The *training error* is the error of our model as calculated on the training dataset; \n",
    "while *generalization error* is the expectation of our model's error when applied to\n",
    "an infinite stream of additional data points\n",
    "drawn from the **same underlying data distribution as our original sample**.\n",
    "\n",
    "##### Bias-Variance Tradeoff\n",
    "\n",
    "(*ML Theory and Statistical Learning*)\n",
    "\n",
    "- **VARIANCE**: \"The amount by which the model **varies** as we change training data is **Variance**\"\n",
    "\n",
    "- **BIAS**: \"The **bias** reflects the amount of **assumptions** we do on the model\"\n",
    "\n",
    "##### Regularisation (against Overfitting)\n",
    "\n",
    "*Weight decay* (commonly called *L2* regularization),\n",
    "might be the most widely-used technique\n",
    "for regularizing parametric machine learning models.\n",
    "\n",
    "In practice, we characterize the regularisation via the *regularisation constant* $\\lambda > 0$, \n",
    "a non-negative hyperparameter that we fit using validation data:\n",
    "\n",
    "$$l(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2.$$\n",
    "\n",
    "For $\\lambda = 0$, we recover our original loss function.\n",
    "For $\\lambda > 0$, we restrict the size of $|| \\mathbf{w} ||$.\n",
    "\n",
    "While L2-regularized linear models constitute\n",
    "the classic *ridge regression* algorithm,\n",
    "L1-regularized linear regression\n",
    "is a similarly fundamental model in statistics\n",
    "(popularly known as *lasso regression*).\n",
    "\n",
    "(More on **Weight Decay** [here](https://github.com/dsgiitr/d2l-pytorch/blob/master/Ch06_Multilayer_Perceptrons/Weight_Decay.ipynb))\n",
    "\n",
    "We will talk about another _technique_ for model regularisation - also known as **structural regularisation**: the **Dropout**\n",
    "\n",
    "[top](#top)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"t8\"></a>\n",
    "\n",
    "### T8. Evaluate your algorithm performance with the Matthews correlation coefficient (MCC) or the Precision-Recall curve\n",
    "\n",
    ">When you apply your trained model to the validation set or to the test set, you need statistical scores to measure your performance.\n",
    "\n",
    "In fact, in a typical supervised binary classification problem, for each element of the validation set (or test set) you have a label stating if the element is `positive` or `negative` (i.e. `1` or `0`). \n",
    "\n",
    "Your machine learning algorithm makes a prediction for each element of the validation set, \n",
    "expressing if it is `1` or `0`, and, based upon these prediction and the gold-standard labels, \n",
    "it will assign each element to one of the following categories: \n",
    "\n",
    "- true negatives (`TN`)  $\\mapsto$ predicted 0; expected 0;\n",
    "- true positives (`TP`)  $\\mapsto$ predicted 1; expected 1; \n",
    "- false positives (`FP`) $\\mapsto$ predicted 1; expected 0; \n",
    "- false negatives (`FN`) $\\mapsto$ predicted 0; expected 1; \n",
    "\n",
    "###### Confusion Matrix \n",
    "\n",
    "<table class=\"wikitable\" style=\"border:none; float:left; margin-top:0;\">\n",
    "<tbody><tr>\n",
    "<th style=\"background:white; border:none;\" colspan=\"2\" rowspan=\"2\">\n",
    "</th>\n",
    "<th colspan=\"2\" style=\"background:none;\">Actual class\n",
    "</th></tr>\n",
    "<tr>\n",
    "<th>P\n",
    "</th>\n",
    "<th>N\n",
    "</th></tr>\n",
    "<tr>\n",
    "<th rowspan=\"2\" style=\"height:6em;\"><div style=\"display: inline-block; -ms-transform: rotate(-90deg); -webkit-transform: rotate(-90deg); transform: rotate(-90deg);;\">Predicted<br>class</div>\n",
    "</th>\n",
    "<th>P\n",
    "</th>\n",
    "<td><b>TP</b>\n",
    "</td>\n",
    "<td>FP\n",
    "</td></tr>\n",
    "<tr>\n",
    "<th>N\n",
    "</th>\n",
    "<td>FN\n",
    "</td>\n",
    "<td><b>TN</b>\n",
    "</td></tr>\n",
    "</tbody></table>\n",
    "\n",
    "**Accuracy**\n",
    "\n",
    "$$\n",
    "accuracy = \\frac{TP+TN}{TP+TN+FP+FN}\n",
    "$$\n",
    "\n",
    "**F1-Score**\n",
    "\n",
    "$$\n",
    "F1 \\; score = \\frac{2 \\cdot TP}{2 \\cdot TP+FP+FN}\n",
    "$$\n",
    "\n",
    "**MCC**\n",
    "\n",
    "$$\n",
    "MCC = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP+FP)\\cdot(TP+FN)\\cdot(TN+FP)\\cdot(TN+FN)}}\n",
    "$$\n",
    "\n",
    "As also reported in [2](#fnmcc), the **most informative metric** to evaluate a confusion matrix is the Matthews correlation coefficient (`MCC`)\n",
    "\n",
    "\n",
    "###### Cases & Flaws:\n",
    "\n",
    "`TP = 90, FP = 5; TN = 1, FN = 4.` $\\mapsto$ OK w/ positive; KO w/ negative\n",
    "\n",
    "`ACC=91%`; `F1=95.24%`; `MCC=0.14`\n",
    "\n",
    "---\n",
    "\n",
    "`TP = 95, FP = 5; TN = 0, FN = 0` $\\mapsto$ Just predicting the majority class (very often the case)\n",
    "\n",
    "`ACC=95%`; `F1=97.44%`; `MCC= **undef** ` (`TF=0` and `TN=0`)\n",
    "\n",
    "\n",
    "<span id=\"fnmcc\"><i>[2]</i>: Chicco D, Jurman G (January 2020). \"The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation\". BMC Genomics. 21 (6). doi:10.1186/s12864-019-6413-7 </span>\n",
    "\n",
    "[top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"t9\"></a>\n",
    "\n",
    "### T9. Program your software with open source code and platforms\n",
    "\n",
    "*in other words:* **DO NOT RE-Invent, but contribute** _plus_\n",
    "\n",
    "![](ds_code.png)\n",
    "\n",
    "[top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"t10\"></a>\n",
    "\n",
    "### T10. Ask for feedback and help to computer science experts, or to collaborative Q&A online communities\n",
    "\n",
    "- Stack Overflow\n",
    "- Open issues on GitHub\n",
    "- Write on mailing list\n",
    "\n",
    "Be polite, and respectful!\n",
    "\n",
    "\n",
    "[top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Extra) T11. Reproducibility and Replicability\n",
    "\n",
    "![](reproducible.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL BioMed)",
   "language": "python",
   "name": "dl-biomed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
